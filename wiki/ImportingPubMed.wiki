#this page details how to use bioruby in order to import data from pubmed articles

= Introduction =

Some more information is needed to be retrieved from the geneRIFs: pubMed articles. As geneRIFs point to the pubMedIDs of articles that describe their functions, some data was obtained from NCBI. 

According to Dr. Simon:
 "For each of the GeneRIF, there is a corresponding pubMed article (indicated by pmID). Using the pmID, we can retrieve the pubMed abstract and related information. 
 At this moment, I am more interested in getting which year was the article published. The publication date can be retrieved by the esummary utility"

esummary utility is described in this address: http://eutils.ncbi.nlm.nih.gov/entrez/query/static/esummary_help.html

In order to retrieve the data some help from Dr. Kibbe was given: ruby scripts!

= Procedures =

Data from esummary utility can be retrieved accessing an URL like this:
http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&id=16609366&retmode=xml

Instead of writing some script code to retrieve and parse urls like that, a faster approach was used. [http://bioruby.org/ BioRuby] is a project aimed to implement environment for Bioinformatics with Ruby language. Main facilities are provided by [http://bioruby.org/rdoc/classes/Bio/MEDLINE.html Medline] and PubMed classes. They are extremely useful for retrieving data like abstract, date of publication, title, authors and so on from pubMed articles. One way to obtain this data would be through the following ruby script:
{{{
#!/usr/bin/env ruby
require 'bio'

entry = Bio::PubMed.query(11024183)     # searches PubMed and get entry
medline = Bio::MEDLINE.new(entry) # creates Bio::MEDLINE object from entry text
reference = medline.reference     # converts into Bio::Reference object
puts reference.bibtex             # shows BibTeX formatted text
}}}
But this function, _Bio::Pubmed.query_ is not as robust as another, _Bio::PubMed.efetch(id)_ , which means, sometimes _Pubmed.query_ returns the right result, and sometimes it just returns an empty query. 

A good tutorial on how to use bioruby is found [http://dev.bioruby.org/wiki/en/?Tutorial.rd here]. In order to use bioruby, firstly it was needed to install ruby (which could be made from repositories) and then [http://bioruby.org/archive/bioruby-1.0.0.tar.gz download] bioruby. To install, in the bioruby source directory (such as bioruby-x.x.x/), run install.rb as follows:
{{{
  % ruby install.rb config
  % ruby install.rb setup
  % su
  # ruby install.rb install
}}}

And then it's done. Since the information requested to pubMed was:
{{{
pmID
PubDate
Source
ISSN
# of authors: count the AuthorList
Title
}}}
a list of pmIDs from generif was firstly generated. All genes are sought, so a query like the following was run over generif database:
{{{
select distinct(pmID) from generif;
}}}
Generating the file out.txt (simple pointing stdout to the file out.txt "sqlite3 gene.db > out.txt").

Then, a simple program was used to slice the results in pieces of 200 pmIDs, since more than 141,000 pmIDs were retrieved. Another point in slicing the results is that queries for esummary made through the Bioruby classes end up making http get connections, so, not too much data may be sent (around 4000 characters).

The simple program to slice ([http://genequad.googlecode.com/svn/wiki/slice.cpp slice.cpp]) files is described here:
{{{
#include <stdio.h>
#include <iostream>
#include <fstream>
#include <string>

using namespace std;

int main(){
  ifstream in("out.txt");
  string s;
  int count=0;
  int fileID=0;
  FILE* out = fopen("out0.txt","w");

  while(getline(in,s)){
    count++;
    if(count==200){
      fprintf(out,"%s",(char*)s.c_str());
      count =0;   
      char temp[200];
      sprintf(temp,"out%d.txt",++fileID);
      out = fopen(temp,"w");      
    }
    else{
      fprintf(out,"%s,",(char*)s.c_str());
    }
  }
  fclose(out);
}
}}}

After compiling this program and running, 707 files were generated (from out0.txt up to out706.txt). Then, it was time for the ruby script start the action. The script [http://genequad.googlecode.com/svn/wiki/getpmIDs.rb getpmIDs.rb] firstly had a simpler version:
{{{
#!/usr/bin/env ruby

require 'bio'

for num in (0..706)
	sleep(1) #sleeps for one second

	myIDs = File.open("out" +num.to_s+".txt").read
	puts 'Reading ' + "out" +num.to_s+".txt"
	myIDs.each{ |line|

		entry = Bio::PubMed.efetch(line.strip)
		entry.each{ |myEntry|
		medline = Bio::MEDLINE.new( myEntry )
		puts medline.pmid
		puts medline.date
		puts medline.source
		puts medline.authors.size
		puts medline.title
		puts medline.is
		puts
		}
	}
end
}}}
But it did not work, because some records did not have ISSNs (so nothing was printed in those lines) and some _source_ descriptions had some '\n' in the middle, so, we would not have one field per line. An enhanced version of [http://genequad.googlecode.com/svn/wiki/getpmIDs.rb getpmIDs.rb] was made:

{{{
#!/usr/bin/env ruby

require 'bio'

for num in (0..706)
	sleep(1) #sleeps for one second

	myIDs = File.open("out" +num.to_s+".txt").read
	puts 'Reading ' + "out" +num.to_s+".txt"
	myIDs.each{ |line|

		entry = Bio::PubMed.efetch(line.strip)
		entry.each{ |myEntry|
		medline = Bio::MEDLINE.new( myEntry )
		puts medline.pmid
		puts medline.date
		puts if medline.date.empty?
		temp = medline.source.gsub(/\n/,'')#removes newlines
		puts temp
		puts if temp.empty?
		puts medline.authors.size
		puts medline.title
		puts if medline.title.empty?
		puts medline.is
		puts if medline.is.empty?
		puts
		}
	}
end
}}}
This version removes '\n' from source as well as prints an empty line when strings are empty. This way, the processed file may be checked for a multiple of 7 lines (6 for each field and 1 for a blank line between pmIDs). After the script was run (what took about 6 hours), the debug lines had to be chopped of the file (debug lines were like "Reading outN.txt"). 

A simple unix command to remove lines that start with "Reading out" is: 
{{{
sed '/^Reading out/d' allPMids.txt > cleanPMids.txt
}}}

This way, a program to check if the file cleanPMids was created, only checking if it followed the pattern 6 lines, blank line, next record. This program is [http://genequad.googlecode.com/svn/wiki/7.cpp 7.cpp]. If the pattern is not found in any of the records, it just breaks out pointing the line for manual cleanup. The code goes like this:
{{{
#include <fstream>
#include <string>
#include <iostream>
#include <vector>
using namespace std;

int main(){
  ifstream in("pubMedsClean.txt");
  string s;
  int count=0;

  while(getline(in,s)){
    count++;
    if(count%7==0){
      if(s!=""){
	printf("Breaking in line %d\n",count);
	break;
      }
    }

  }
}
}}}

Only four or five manual adjustments were needed running this program. All of them was because _source_ field was missing. After it was corrected, another program took place to insert data into generif database. But firstly, another table was created in the same database that stored generif. It was created with the following sql commands:

{{{
create table pubMed (
pmID integer primary key,
pubDate varchar(20),
source text,
numAuthors integer,
title text,
issn varchar (28)
);
}}}

After that, [http://genequad.googlecode.com/svn/wiki/insertpmIDs.cpp insertpmIDs.cpp] was compiled (with -lsqlite3 flag) and run. The process of inserting this amount of records took a couple hours. The code from insertpmIDs is as follows:
{{{
#include <fstream>
#include <string>
#include <iostream>
#include <vector>
#include <sqlite3.h>

using namespace std;

string replace(string test){
  string ans;
  string::size_type pos=0;
  while( (pos = test.find("\'",pos))<test.size() ){
    test.replace(pos,1,"\'\'");
    pos = pos + 2;
  }
  ans = test;
  return ans;  
}

int main(){
  ifstream in("cleanPMids.txt");
  vector<string> fields(7);
  string::size_type pos = 0;

  sqlite3 *db;
  char* zErrMsg = 0;
  int rc = sqlite3_open("gene.db",&db);
  if(rc) {printf("bugged\n");return 0;}

  int count=0;

  while(1){
    fields.clear();
    for(int i=0;i<7;i++)      
      getline(in,fields[i]);

    if(fields[0]=="") break;
  

    char temp[20000];
    
    sprintf(temp,"insert into pubMed values(%s , '%s' , '%s' , %s , '%s', '%s')",
	    replace(fields[0]).c_str(),//pmID
	    replace(fields[1]).c_str(),//pubDate
	    replace(fields[2]).c_str(),//source
	    replace(fields[3]).c_str(),//number of authors
	    replace(fields[4]).c_str(),//paper title
	    replace(fields[5]).c_str() //issn
	    );

    rc = sqlite3_exec(db,temp, NULL, 0, &zErrMsg);
    if( rc!=SQLITE_OK){
      fprintf(stderr, "SQL error: %s\n",zErrMsg);
      fprintf(stderr, "Blamed string: %s\n",temp);
      sqlite3_free(zErrMsg);
      }
    printf("%s\n",(char*)fields[0].c_str());

  }
  sqlite3_close(db);
}
}}}

= Validation =

One way to validate the results is to run the same query for distinct pmIDs in both tables  and expect the same answer:

{{{
sqlite> select count(distinct(pmID)) from generif;
141392
sqlite> select count(distinct(pmID)) from pubmed;
141392
}}}

If they are different - as happened the first time this query was done - one might need to query for which pmIDs are missing:
{{{
select generif.pmID from generif  where generif.pmID not in(select distinct pubmed.pmID from pubmed);
}}}
And repeat the slicing procedure for these records. Around 200 records needed to be re-queried in the first time this import was done.